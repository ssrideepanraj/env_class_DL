{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d00846b-dfc5-40fb-bc2a-1ed660a37823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import random_uniform, glorot_uniform\n",
    "from skimage import measure\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73bbad03-0b0c-452a-9a05-c1b1477bf067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractal_dimension(img):\n",
    "    img = np.array(img.convert('L'))  # Convert to grayscale\n",
    "    threshold = 128\n",
    "    binary_img = img > threshold  # Binary image based on threshold\n",
    "\n",
    "    def boxcount(binary_img, box_size):\n",
    "        S = np.add.reduceat(\n",
    "            np.add.reduceat(binary_img, np.arange(0, binary_img.shape[0], box_size), axis=0),\n",
    "            np.arange(0, binary_img.shape[1], box_size), axis=1)\n",
    "        return len(np.where(S > 0)[0])\n",
    "\n",
    "    sizes = [2, 4, 8, 16, 32]\n",
    "    counts = []\n",
    "    for size in sizes:\n",
    "        count = boxcount(binary_img, size)\n",
    "        counts.append(count)\n",
    "        #print(f\"Size: {size}, Count: {count}\")  # Debug output\n",
    "\n",
    "    # Convert sizes and counts to numpy arrays for processing\n",
    "    sizes = np.array(sizes)\n",
    "    counts = np.array(counts)\n",
    "\n",
    "    # Filter out zero values for log calculation\n",
    "    non_zero_indices = (sizes > 0) & (counts > 0)\n",
    "    filtered_sizes = sizes[non_zero_indices]\n",
    "    filtered_counts = counts[non_zero_indices]\n",
    "\n",
    "    # Debug output for filtered sizes and counts\n",
    "\n",
    "    #print(f\"Filtered Sizes: {filtered_sizes}, Filtered Counts: {filtered_counts}\")\n",
    "\n",
    "    if filtered_sizes.size > 0 and filtered_counts.size > 0:\n",
    "        coeffs = np.polyfit(np.log(filtered_sizes), np.log(filtered_counts), 1)\n",
    "        return -coeffs[0]\n",
    "    else:\n",
    "        #print(\"No valid sizes or counts for fractal dimension calculation.\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc306bf3-be0c-4b88-9254-b41ea775455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "dataset_url = r'E:\\S7 - PROJECT WORK\\2750'\n",
    "batch_size = 32\n",
    "img_height, img_width = 64, 64\n",
    "validation_split = 0.2\n",
    "rescale = 1.0 / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66edc1cf-8527-437b-8c7e-e5f459546028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_image(img_path, target_size=(64, 64)):\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = img_array / 255.0  # Normalize\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438455be-3cbc-44a9-b899-1f86cee1a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for the model\n",
    "fractal_features = []\n",
    "image_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60a6f804-284d-4bc2-b318-e590c911d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all images to calculate fractal features\n",
    "for file in glob.glob(os.path.join(dataset_url, '*/*.jpg')):\n",
    "    img_array = load_and_preprocess_image(file)  # Get image array\n",
    "    image_data.append(img_array)\n",
    "    \n",
    "    fractal_dim = fractal_dimension(load_img(file))  # Calculate fractal dimension\n",
    "    fractal_features.append(fractal_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915716a4-3944-411b-ae86-6a0f0b854280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to NumPy arrays\n",
    "image_data = np.array(image_data)  # Shape: (num_images, height, width, channels)\n",
    "fractal_features = np.array(fractal_features)  # Shape: (num_images,)\n",
    "\n",
    "# Reshape fractal_features to be compatible for concatenation\n",
    "# Here we reshape to (num_images, 1) to prepare for expansion\n",
    "fractal_features_reshaped = fractal_features.reshape(-1, 1)  # Shape: (num_images, 1)\n",
    "\n",
    "# Expand fractal features to match the height and width of the image data\n",
    "# Create an array with the same height and width as the images\n",
    "# This will create an array of shape (num_images, height, width)\n",
    "fractal_features_expanded = np.tile(fractal_features_reshaped[:, np.newaxis], (1, image_data.shape[1], image_data.shape[2]))  # Shape: (num_images, height, width)\n",
    "\n",
    "# Add a new axis to make it (num_images, height, width, 1)\n",
    "fractal_features_expanded = fractal_features_expanded[:, :, :, np.newaxis]  # Shape: (num_images, height, width, 1)\n",
    "\n",
    "# Combine image data and fractal features\n",
    "combined_data = np.concatenate((image_data, fractal_features_expanded), axis=-1)  # Shape: (num_images, height, width, channels + 1)\n",
    "\n",
    "# Now combined_data can be used for training your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "039f78b6-745f-4345-b59b-17de05ef2a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27000, 64, 64, 4)\n"
     ]
    }
   ],
   "source": [
    "print(combined_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c966f173-65ef-4f7b-980a-9d7fa3daf872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, training=True, initializer=glorot_uniform):\n",
    "    F1, F2, F3 = filters\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters=F1, kernel_size=1, strides=(1, 1), padding='valid', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X, training=training)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X, training=training)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X, training=training)\n",
    "\n",
    "    X = Add()([X_shortcut, X])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b47179e-d2d7-4960-9352-db02b38f1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, s=2, training=True, initializer=glorot_uniform):\n",
    "    F1, F2, F3 = filters\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters=F1, kernel_size=1, strides=(s, s), padding='valid', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X, training=training)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(F2, (f, f), strides=(1, 1), padding='same', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X, training=training)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(F3, (1, 1), strides=(1, 1), padding='valid', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X, training=training)\n",
    "\n",
    "    X_shortcut = Conv2D(F3, (1, 1), strides=(s, s), padding='valid', kernel_initializer=initializer(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3)(X_shortcut, training=training)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "064b630b-63c7-46d7-a19b-4a6e20560fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(input_shape=(64, 64, 4), classes=10):  # Adjust input shape for fractal features\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    X = Conv2D(64, (7, 7), strides=(2, 2), kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[64, 64, 256], s=1)\n",
    "    X = identity_block(X, 3, [64, 64, 256])\n",
    "    X = identity_block(X, 3, [64, 64, 256])\n",
    "    \n",
    "    X = convolutional_block(X, f=3, filters=[128, 128, 512], s=2)\n",
    "    X = identity_block(X, 3, [128, 128, 512])\n",
    "    X = identity_block(X, 3, [128, 128, 512])\n",
    "    X = identity_block(X, 3, [128, 128, 512])\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], s=2)\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[512, 512, 2048], s=2)\n",
    "    X = identity_block(X, 3, [512, 512, 2048])\n",
    "    X = identity_block(X, 3, [512, 512, 2048])\n",
    "\n",
    "    X = AveragePooling2D(pool_size=(2, 2), name='avg_pool')(X)\n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    model = Model(inputs=X_input, outputs=X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4735df82-2a38-402a-a953-4c7bb796ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model = ResNet50(input_shape=(64, 64, 4), classes=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fd400b9-6fed-43b8-a77c-cfde5e5e7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted input shape if adding fractal features\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31a17e80-e8c4-40b5-848b-ca9f624f278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, MaxPooling2D, AveragePooling2D, Flatten, Dense, ZeroPadding2D\n",
    "from keras.models import Model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import to_categorical  # Importing to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87b06502-a733-475e-8205-e39617a12fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your dataset (replace this with your actual data)\n",
    "combined_data = np.random.rand(27000, 64, 64, 4)  # Example placeholder for combined data\n",
    "num_classes = 10  # Adjust this according to your dataset\n",
    "labels = np.array([np.random.randint(0, num_classes) for _ in range(27000)])  # Replace with your actual labels\n",
    "labels_one_hot = to_categorical(labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4128f827-f3a1-47db-a5b0-7cffa34e3839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure labels shape is correct\n",
    "assert labels_one_hot.shape[0] == combined_data.shape[0], \"Labels and data size mismatch!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35beb2ff-838f-44b3-8986-fe900b5e6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for early stopping and model checkpointing\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('bestfrac_model.keras', save_best_only=True, monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09f31f48-fbfe-4d54-8be3-9d92a3ae4cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 670ms/step - accuracy: 0.0999 - loss: 2.7300 - val_accuracy: 0.0972 - val_loss: 2.9586\n",
      "Epoch 2/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 656ms/step - accuracy: 0.1056 - loss: 2.5161 - val_accuracy: 0.1026 - val_loss: 2.6222\n",
      "Epoch 3/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 655ms/step - accuracy: 0.1110 - loss: 2.4868 - val_accuracy: 0.0996 - val_loss: 2.7664\n",
      "Epoch 4/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 651ms/step - accuracy: 0.1175 - loss: 2.4818 - val_accuracy: 0.0994 - val_loss: 2.4786\n",
      "Epoch 5/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 645ms/step - accuracy: 0.1241 - loss: 2.4094 - val_accuracy: 0.1015 - val_loss: 2.4129\n",
      "Epoch 6/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 646ms/step - accuracy: 0.1282 - loss: 2.3773 - val_accuracy: 0.1087 - val_loss: 2.4208\n",
      "Epoch 7/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 645ms/step - accuracy: 0.1484 - loss: 2.3360 - val_accuracy: 0.0959 - val_loss: 2.6372\n",
      "Epoch 8/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 641ms/step - accuracy: 0.1699 - loss: 2.2981 - val_accuracy: 0.1011 - val_loss: 2.4350\n",
      "Epoch 9/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 640ms/step - accuracy: 0.1820 - loss: 2.2540 - val_accuracy: 0.1059 - val_loss: 2.5676\n",
      "Epoch 10/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 653ms/step - accuracy: 0.2155 - loss: 2.1736 - val_accuracy: 0.1007 - val_loss: 2.7083\n",
      "Epoch 11/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 656ms/step - accuracy: 0.2625 - loss: 2.0505 - val_accuracy: 0.1020 - val_loss: 2.5993\n",
      "Epoch 12/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 658ms/step - accuracy: 0.3106 - loss: 1.9391 - val_accuracy: 0.0931 - val_loss: 2.6691\n",
      "Epoch 13/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 655ms/step - accuracy: 0.3783 - loss: 1.7596 - val_accuracy: 0.0996 - val_loss: 2.9819\n",
      "Epoch 14/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 660ms/step - accuracy: 0.4630 - loss: 1.5453 - val_accuracy: 0.0959 - val_loss: 3.4340\n",
      "Epoch 15/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 657ms/step - accuracy: 0.5458 - loss: 1.3031 - val_accuracy: 0.1004 - val_loss: 3.2502\n"
     ]
    }
   ],
   "source": [
    "# Train the model with callbacks\n",
    "history = model.fit(combined_data, labels_one_hot, validation_split=0.2, epochs=25, batch_size=32,\n",
    "                    callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c339f99-599b-408b-ae77-9ecfd064a737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m770s\u001b[0m 1s/step - accuracy: 0.1287 - loss: 2.4493 - val_accuracy: 0.1022 - val_loss: 2.5540\n",
      "Epoch 17/25\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 1s/step - accuracy: 0.1220 - loss: 2.4118 - val_accuracy: 0.0967 - val_loss: 2.4242\n",
      "Epoch 18/25\n",
      "\u001b[1m328/675\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:08:30\u001b[0m 22s/step - accuracy: 0.1521 - loss: 2.3389"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the last best model checkpoint saved\n",
    "model = load_model('bestfrac_model.keras')\n",
    "\n",
    "# Re-compile the model if necessary\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks for resuming training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('bestfrac_model.keras', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Resume training\n",
    "history = model.fit(\n",
    "    combined_data, labels_one_hot, \n",
    "    validation_split=0.2, \n",
    "    initial_epoch=15,  # Start from epoch 15\n",
    "    epochs=25,         # Continue up to the 25th epoch\n",
    "    batch_size=32, \n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1685744-7b5d-446a-b412-0a1f29dba716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a127e18-2f63-41c2-8832-91fa1c51d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the last best model checkpoint saved\n",
    "model = load_model('bestfrac_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b288a6f3-c819-4229-b4e8-c67fc32f4fdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(\u001b[43mcombined_data\u001b[49m, labels_one_hot, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combined_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(combined_data, labels_one_hot, verbose=1)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e80c06-73fb-415b-b777-2476921a67c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
